{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1670472818051,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "yu0OV4sf1nk8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670472819084,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "svhuDSYP1nk-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "import urllib.request\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "from IPython.core.display import HTML\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import tqdm\n",
    "from itertools import groupby\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670472822205,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "vGhXf6lC1nk_",
    "outputId": "478747bb-6f8c-481d-ebb4-82b506528c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.11.0\n",
      "keras version 2.11.0\n",
      "Eager Execution Enabled: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of replicas: 1\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtMgghJO1nlA"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "executionInfo": {
     "elapsed": 7156,
     "status": "ok",
     "timestamp": 1670461970917,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "eSg0NnAgXpRg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !unzip \"./all_data/sep_1/data.zip\" -d \"./temp_data/sep_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670472827844,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "tbOPJ2jS1nlB"
   },
   "outputs": [],
   "source": [
    "# Getting the folder names \n",
    "folder_names_1 = glob.glob(\"./temp_data/sep_1/data/*\")\n",
    "folder_names_2 = glob.glob(\"./temp_data/sep_0.7/data/*\")\n",
    "# folder_names = folder_names_1 + folder_names_2\n",
    "# folder_names = folder_names_1\n",
    "folder_names = folder_names_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670472829094,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "JkZ1ScX01nlB"
   },
   "outputs": [],
   "source": [
    "# Getting the data names\n",
    "data_names = []\n",
    "for i in folder_names:\n",
    "    data_names.append(glob.glob(i+\"/*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng5z05Q61nlC"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "executionInfo": {
     "elapsed": 14851,
     "status": "ok",
     "timestamp": 1670472843943,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "06RUU7IU1nlD"
   },
   "outputs": [],
   "source": [
    "# Datasets to skip \n",
    "skip = []\n",
    "\n",
    "# Labels of the datasets\n",
    "labels = []\n",
    "\n",
    "# Data list \n",
    "data = []\n",
    "\n",
    "\n",
    "# Getting the labels and data for each dataset\n",
    "for i in range(len(data_names)):\n",
    "    if i in skip:\n",
    "        continue\n",
    "    \n",
    "    for j in range(len(data_names[i])):\n",
    "        labels.append([data_names[i][j][data_names[i][j].find(\".csv\")-1]])\n",
    "        \n",
    "        # Cleaning data\n",
    "        df = pd.read_csv(data_names[i][j],skiprows = 1)\n",
    "        df.drop(columns=df.columns[-1], axis=1,  inplace=True)\n",
    "        \n",
    "        data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670472843943,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "OhMZiWrx1nlD"
   },
   "outputs": [],
   "source": [
    "# Changing the labels from int to string\n",
    "labels = [int(i)-1 for i in np.reshape(labels,(-1,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets for sep_1\n",
    "target = np.array([[-0.314,1.661,0.45],[0,1.661,0.45],[0.314,1.661,0.45],[-0.314,1.347,0.45],[0,1.347,0.45],[0.314,1.347,0.45],[-0.314,1.033,0.45],[0,1.033,0.45],[0.314,1.033,0.45]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3aWbcBR1nlD"
   },
   "source": [
    "## Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "executionInfo": {
     "elapsed": 57502,
     "status": "ok",
     "timestamp": 1670472901442,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "6sJpWHrX1nlE"
   },
   "outputs": [],
   "source": [
    "# Adding the centroid of all the fingers to the data\n",
    "\n",
    "new_data = []\n",
    "\n",
    "for idx_1 in range(len(data)):\n",
    "\n",
    "    \n",
    "    # Grouped columns for centroid\n",
    "    grouped_columns_x = data[idx_1].columns[3::3] \n",
    "    grouped_columns_y = data[idx_1].columns[4::3]\n",
    "    grouped_columns_z = data[idx_1].columns[5::3]\n",
    "    \n",
    "    # Getting the centroid of the finger points\n",
    "    cent_x = np.mean(data[idx_1][grouped_columns_x],axis = 1)\n",
    "    cent_y = np.mean(data[idx_1][grouped_columns_y],axis = 1)\n",
    "    cent_z = np.mean(data[idx_1][grouped_columns_z],axis = 1)\n",
    "    \n",
    "    new_data.append(pd.concat([data[idx_1],cent_x,cent_y,cent_z],axis = 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgaA_2SY0MDc"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670472901442,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "MUjsQWtJsKIn"
   },
   "outputs": [],
   "source": [
    "# Pulling only the centroid data \n",
    "cent_data = [i.iloc[:,-3:] for i in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1670472902684,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "cq550A880PAk",
    "outputId": "8c0ba4d0-9ee8-4885-fe2b-e9f624183928"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = np.array([len(i) for i in cent_data])\n",
    "plt.hist(lengths)\n",
    "plt.xlabel(\"Trajectory length\")\n",
    "plt.ylabel(\"Number of trajectories\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.61878727634195"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the mean trajectory lengths\n",
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670472902685,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "ymWVIli90Uu8"
   },
   "outputs": [],
   "source": [
    "# Removing trajectories with <=5 length\n",
    "idx = np.where(lengths <= 13.)[0]\n",
    "[labels.pop(j-i) for i,j in enumerate(idx)]\n",
    "[cent_data.pop(j-i) for i,j in enumerate(idx)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670472902685,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "9X0N0OYxcyyJ",
    "outputId": "e7d207da-0b10-4219-ad7d-a7ab34848596"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if it worked \n",
    "lengths = np.array([len(i) for i in cent_data])\n",
    "np.where(lengths <= 13.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For target point prediction\n",
    "data_3d = []\n",
    "target_3d = []\n",
    "for i in cent_data:\n",
    "    data_3d.append(i.iloc[:-10,:])\n",
    "    temp = []\n",
    "    for j in range(len(i.iloc[:-10,:])):\n",
    "        temp.append(i.iloc[j+1:j+11,:])\n",
    "    target_3d.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670472902685,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "hfPRW0S7UN4b"
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(cent_data, labels, test_size=0.2, random_state = 7)\n",
    "# x_train_, x_test_, y_train_, y_test_ = train_test_split(data_3d, target_3d, test_size=0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670472902687,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "kKLJbGtv1nlE"
   },
   "outputs": [],
   "source": [
    "# Getting the data in the required format for the model\n",
    "# Creating labels for RNN's output \n",
    "y_train = [[dt]*200 for idx,dt in enumerate(y_train)]\n",
    "y_test = [[dt]*200 for idx,dt in enumerate(y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670472902688,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "evCXyI6-1nlE"
   },
   "outputs": [],
   "source": [
    "# Sequence length is based on data analysis\n",
    "# x_train = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(x_train_, padding='post', dtype='float', maxlen=200, value = -10.))\n",
    "x_train = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(x_train, padding='post', dtype='float', maxlen=200, value = 0))\n",
    "# y_train = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(y_train_, padding='post', dtype='float', maxlen=200))\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "# x_test = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(x_test_, padding='post', dtype='float', maxlen=200, value = -10.))\n",
    "x_test = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(x_test, padding='post', dtype='float', maxlen=200, value = 0))\n",
    "# y_test = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(y_test_, padding='post', dtype='float', maxlen=200))\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670472902688,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "ryFy62Mk1nlF"
   },
   "outputs": [],
   "source": [
    "# One hot encoding the data for training - not required for label_3d\n",
    "y_train = tf.one_hot(y_train, 9, on_value = 1.0, off_value = 0.0)\n",
    "y_test = tf.one_hot(y_test, 9, on_value = 1.0, off_value = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJc39yrs1nlE"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DTW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dtw(ip_data,op_data):\n",
    "    data_dict_x = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "    data_dict_y = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "    data_dict_z = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "\n",
    "    time = np.arange(len(ip_data))\n",
    "    for i,j in zip(ip_data,op_data):\n",
    "\n",
    "        time = np.arange(len(i))\n",
    "        \n",
    "        data_dict_x[j].append(np.polyfit(time,i[0],3))\n",
    "        data_dict_y[j].append(np.polyfit(time,i[1],3))\n",
    "        data_dict_z[j].append(np.polyfit(time,i[2],3))\n",
    "\n",
    "    # getting the mean and trend\n",
    "    trend_dict_x = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "    trend_dict_y = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "    trend_dict_z = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "    time = np.arange(70)\n",
    "    for i in range(9):\n",
    "        trend_dict_x[i] = np.poly1d(np.mean(data_dict_x[i],axis = 0))(time)\n",
    "        trend_dict_y[i] = np.poly1d(np.mean(data_dict_y[i],axis = 0))(time)\n",
    "        trend_dict_z[i] = np.poly1d(np.mean(data_dict_z[i],axis = 0))(time)\n",
    "\n",
    "  \n",
    "    dict_q = []\n",
    "    for i in range(9):\n",
    "        dict_q.append(np.array([trend_dict_x[i],trend_dict_y[i],trend_dict_z[i]]))\n",
    "    return dict_q\n",
    "\n",
    "\n",
    "def model_dtw_pred(ip_data,dic):\n",
    "\n",
    "    \n",
    "    predi = []\n",
    "    d = []\n",
    "    for i in range(0,9):\n",
    "        distance, path = fastdtw(dic[i].T, ip_data, dist=euclidean)\n",
    "        d.append(distance)\n",
    "\n",
    "    predi.append(d.index(min(d)))\n",
    "    \n",
    "    return predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1605/1605 [02:06<00:00, 12.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# dict_q = model_dtw(x_train,y_train)\n",
    "# with open(\"./models/sep_1/data_driven\", 'wb') as handle:\n",
    "#     pickle.dump(dict_q, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Load data (deserialize)\n",
    "with open('./models/sep_0.7/data_driven', 'rb') as handle:\n",
    "    dic = pickle.load(handle)\n",
    "\n",
    "preds = [model_dtw_pred(i,dic)[0] for i in tqdm.tqdm(x_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115264797507788"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.array(preds) == np.array(y_train))*1)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = []\n",
    "# for i in range(9):\n",
    "#     temp.append(dict_q[i].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(9):\n",
    "#     pd.DataFrame(temp[i]).to_csv(\"./\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670472902689,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "LJC8EL0egiGL"
   },
   "outputs": [],
   "source": [
    "hidden_size= 15\n",
    "tf.keras.backend.clear_session()\n",
    "seq_input = tf.keras.Input(shape=x_train.shape[1:], name='input_layer')\n",
    "\n",
    "ip_reform = tf.keras.layers.Dense(hidden_size)\n",
    "\n",
    "ip_reformed = ip_reform(seq_input)\n",
    "\n",
    "LSTM = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "h = LSTM(ip_reformed)\n",
    "\n",
    "dense = tf.keras.layers.Dense(9, activation='softmax')\n",
    "output = dense(h)\n",
    "\n",
    "model_LSTM = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "\n",
    "model_LSTM.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer=Adam(1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM model - labels_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size= 15\n",
    "tf.keras.backend.clear_session()\n",
    "seq_input = tf.keras.Input(shape=x_train.shape[1:], name='input_layer')\n",
    "\n",
    "masking = tf.keras.layers.Masking(mask_value=-10.)\n",
    "masked_op = masking(seq_input)\n",
    "\n",
    "ip_reform = tf.keras.layers.Dense(hidden_size)\n",
    "\n",
    "ip_reformed = ip_reform(masked_op)\n",
    "\n",
    "LSTM = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "h = LSTM(ip_reformed)\n",
    "\n",
    "dense = tf.keras.layers.Dense(30, activation='linear')\n",
    "dense_out = dense(h)\n",
    "\n",
    "reshaping = tf.keras.layers.Reshape((200,10, 3))\n",
    "output = reshaping(dense_out)\n",
    "\n",
    "model_LSTM_3d = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "\n",
    "model_LSTM_3d.compile(loss='mse', metrics = ['mae'],optimizer=Adam(1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM+RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670472905553,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "qsyhNfUNdb9D"
   },
   "outputs": [],
   "source": [
    "hidden_size= 15\n",
    "tf.keras.backend.clear_session()\n",
    "seq_input = tf.keras.Input(shape=x_train.shape[1:], name='input_layer')\n",
    "\n",
    "ip_reform = tf.keras.layers.Dense(hidden_size)\n",
    "\n",
    "ip_reformed = ip_reform(seq_input)\n",
    "\n",
    "RNN = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True)\n",
    "h_1 = RNN(ip_reformed)\n",
    "\n",
    "LSTM = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "h_2 = LSTM(h_1)\n",
    "\n",
    "dense = tf.keras.layers.Dense(9, activation='softmax')\n",
    "output = dense(h_2)\n",
    "\n",
    "model_RNN_LSTM = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "\n",
    "model_RNN_LSTM.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer=Adam(1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LSTM+RNN model - labels_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size= 15\n",
    "tf.keras.backend.clear_session()\n",
    "seq_input = tf.keras.Input(shape=x_train.shape[1:], name='input_layer')\n",
    "\n",
    "masking = tf.keras.layers.Masking(mask_value=-10.)\n",
    "masked_op = masking(seq_input)\n",
    "\n",
    "ip_reform = tf.keras.layers.Dense(hidden_size)\n",
    "\n",
    "ip_reformed = ip_reform(masked_op)\n",
    "\n",
    "RNN = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True)\n",
    "h_1 = RNN(ip_reformed)\n",
    "\n",
    "LSTM = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "h_2 = LSTM(h_1)\n",
    "\n",
    "dense = tf.keras.layers.Dense(30, activation='linear')\n",
    "dense_out = dense(h_2)\n",
    "\n",
    "reshaping = tf.keras.layers.Reshape((200,10, 3))\n",
    "output = reshaping(dense_out)\n",
    "\n",
    "model_RNN_LSTM_3d = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "\n",
    "model_RNN_LSTM_3d.compile(loss='mse', metrics = ['mae'], optimizer=Adam(1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670472905553,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "29kaMRnt1nlF",
    "outputId": "96eed065-c571-49cd-fffa-6e4ef9d4d358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 200, 3)]          0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, 200, 3)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200, 15)           60        \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 200, 15)           465       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 15)           1860      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200, 30)           480       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 200, 10, 3)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,865\n",
      "Trainable params: 2,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN_LSTM_3d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1593,
     "status": "ok",
     "timestamp": 1670472927009,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "7B3g6dUGOOoU"
   },
   "outputs": [],
   "source": [
    "# callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_accuracy',\n",
    "#     patience=100,\n",
    "#     restore_best_weights = True\n",
    "# )\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=100,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdnYYPv_1nlF",
    "outputId": "a1bb33a5-8b8c-446d-9016-a14cbc846054",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 16:35:11.906640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2022-12-13 16:35:14.020076: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x14a091c54b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-13 16:35:14.020095: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2022-12-13 16:35:14.103729: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-13 16:35:14.610576: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 34s 149ms/step - loss: 0.0109 - mae: 0.0405 - val_loss: 0.0021 - val_mae: 0.0182\n",
      "Epoch 2/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 0.0014 - mae: 0.0148 - val_loss: 9.0127e-04 - val_mae: 0.0121\n",
      "Epoch 3/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 6.3876e-04 - mae: 0.0100 - val_loss: 7.9560e-04 - val_mae: 0.0119\n",
      "Epoch 4/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 5.3815e-04 - mae: 0.0091 - val_loss: 5.1718e-04 - val_mae: 0.0089\n",
      "Epoch 5/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 3.3984e-04 - mae: 0.0071 - val_loss: 2.5057e-04 - val_mae: 0.0058\n",
      "Epoch 6/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 2.4240e-04 - mae: 0.0058 - val_loss: 3.7125e-04 - val_mae: 0.0076\n",
      "Epoch 7/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 2.4036e-04 - mae: 0.0057 - val_loss: 2.9518e-04 - val_mae: 0.0066\n",
      "Epoch 8/1000\n",
      "116/116 [==============================] - 16s 138ms/step - loss: 2.2973e-04 - mae: 0.0056 - val_loss: 2.1140e-04 - val_mae: 0.0050\n",
      "Epoch 9/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 2.1275e-04 - mae: 0.0054 - val_loss: 2.5746e-04 - val_mae: 0.0058\n",
      "Epoch 10/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 2.2137e-04 - mae: 0.0055 - val_loss: 2.6234e-04 - val_mae: 0.0061\n",
      "Epoch 11/1000\n",
      "116/116 [==============================] - 15s 132ms/step - loss: 2.2524e-04 - mae: 0.0057 - val_loss: 2.2886e-04 - val_mae: 0.0054\n",
      "Epoch 12/1000\n",
      "116/116 [==============================] - 16s 135ms/step - loss: 2.1259e-04 - mae: 0.0054 - val_loss: 2.3523e-04 - val_mae: 0.0056\n",
      "Epoch 13/1000\n",
      "116/116 [==============================] - 16s 137ms/step - loss: 2.1863e-04 - mae: 0.0056 - val_loss: 2.3231e-04 - val_mae: 0.0058\n",
      "Epoch 14/1000\n",
      "116/116 [==============================] - 16s 138ms/step - loss: 2.0749e-04 - mae: 0.0055 - val_loss: 2.3547e-04 - val_mae: 0.0058\n",
      "Epoch 15/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 1.9315e-04 - mae: 0.0052 - val_loss: 2.3251e-04 - val_mae: 0.0059\n",
      "Epoch 16/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 2.2158e-04 - mae: 0.0058 - val_loss: 3.2447e-04 - val_mae: 0.0082\n",
      "Epoch 17/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 2.0510e-04 - mae: 0.0054 - val_loss: 1.9539e-04 - val_mae: 0.0050\n",
      "Epoch 18/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.8087e-04 - mae: 0.0050 - val_loss: 1.8611e-04 - val_mae: 0.0050\n",
      "Epoch 19/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 2.0454e-04 - mae: 0.0055 - val_loss: 2.4169e-04 - val_mae: 0.0065\n",
      "Epoch 20/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 2.3520e-04 - mae: 0.0061 - val_loss: 2.1508e-04 - val_mae: 0.0055\n",
      "Epoch 21/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.9685e-04 - mae: 0.0055 - val_loss: 1.8574e-04 - val_mae: 0.0049\n",
      "Epoch 22/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 1.8268e-04 - mae: 0.0052 - val_loss: 1.9306e-04 - val_mae: 0.0053\n",
      "Epoch 23/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.7663e-04 - mae: 0.0050 - val_loss: 1.7536e-04 - val_mae: 0.0047\n",
      "Epoch 24/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 1.6423e-04 - mae: 0.0048 - val_loss: 1.7106e-04 - val_mae: 0.0048\n",
      "Epoch 25/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.6532e-04 - mae: 0.0048 - val_loss: 1.6487e-04 - val_mae: 0.0045\n",
      "Epoch 26/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 1.6102e-04 - mae: 0.0048 - val_loss: 1.7933e-04 - val_mae: 0.0051\n",
      "Epoch 27/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 1.8027e-04 - mae: 0.0052 - val_loss: 1.6522e-04 - val_mae: 0.0045\n",
      "Epoch 28/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.5161e-04 - mae: 0.0046 - val_loss: 1.6904e-04 - val_mae: 0.0047\n",
      "Epoch 29/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.6381e-04 - mae: 0.0050 - val_loss: 1.8683e-04 - val_mae: 0.0055\n",
      "Epoch 30/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.6783e-04 - mae: 0.0051 - val_loss: 1.8895e-04 - val_mae: 0.0051\n",
      "Epoch 31/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.6812e-04 - mae: 0.0051 - val_loss: 1.7076e-04 - val_mae: 0.0053\n",
      "Epoch 32/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 1.5477e-04 - mae: 0.0049 - val_loss: 1.7758e-04 - val_mae: 0.0057\n",
      "Epoch 33/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.5530e-04 - mae: 0.0051 - val_loss: 2.5091e-04 - val_mae: 0.0071\n",
      "Epoch 34/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 1.5556e-04 - mae: 0.0050 - val_loss: 2.0296e-04 - val_mae: 0.0066\n",
      "Epoch 35/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 1.3013e-04 - mae: 0.0044 - val_loss: 1.6148e-04 - val_mae: 0.0045\n",
      "Epoch 36/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 1.4532e-04 - mae: 0.0048 - val_loss: 1.2773e-04 - val_mae: 0.0040\n",
      "Epoch 37/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.3465e-04 - mae: 0.0046 - val_loss: 1.8769e-04 - val_mae: 0.0056\n",
      "Epoch 38/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.2977e-04 - mae: 0.0045 - val_loss: 1.6856e-04 - val_mae: 0.0054\n",
      "Epoch 39/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.2616e-04 - mae: 0.0043 - val_loss: 1.2191e-04 - val_mae: 0.0038\n",
      "Epoch 40/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 1.3013e-04 - mae: 0.0044 - val_loss: 2.1867e-04 - val_mae: 0.0070\n",
      "Epoch 41/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 1.3100e-04 - mae: 0.0045 - val_loss: 1.2874e-04 - val_mae: 0.0044\n",
      "Epoch 42/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.2824e-04 - mae: 0.0045 - val_loss: 1.2069e-04 - val_mae: 0.0041\n",
      "Epoch 43/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 1.1294e-04 - mae: 0.0041 - val_loss: 1.3843e-04 - val_mae: 0.0047\n",
      "Epoch 44/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 1.1907e-04 - mae: 0.0042 - val_loss: 2.0052e-04 - val_mae: 0.0055\n",
      "Epoch 45/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 1.2231e-04 - mae: 0.0044 - val_loss: 1.4533e-04 - val_mae: 0.0050\n",
      "Epoch 46/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 1.0901e-04 - mae: 0.0041 - val_loss: 1.1566e-04 - val_mae: 0.0040\n",
      "Epoch 47/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 1.0791e-04 - mae: 0.0040 - val_loss: 1.1435e-04 - val_mae: 0.0038\n",
      "Epoch 48/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.0649e-04 - mae: 0.0040 - val_loss: 1.4358e-04 - val_mae: 0.0046\n",
      "Epoch 49/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 1.1126e-04 - mae: 0.0041 - val_loss: 1.0613e-04 - val_mae: 0.0037\n",
      "Epoch 50/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 9.1794e-05 - mae: 0.0037 - val_loss: 1.1627e-04 - val_mae: 0.0043\n",
      "Epoch 51/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 9.0369e-05 - mae: 0.0036 - val_loss: 1.2046e-04 - val_mae: 0.0043\n",
      "Epoch 52/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 9.4732e-05 - mae: 0.0038 - val_loss: 1.0782e-04 - val_mae: 0.0041\n",
      "Epoch 53/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 9.1811e-05 - mae: 0.0038 - val_loss: 1.0530e-04 - val_mae: 0.0038\n",
      "Epoch 54/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 9.0454e-05 - mae: 0.0037 - val_loss: 1.1607e-04 - val_mae: 0.0043\n",
      "Epoch 55/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 9.4180e-05 - mae: 0.0037 - val_loss: 1.0110e-04 - val_mae: 0.0038\n",
      "Epoch 56/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 8.0517e-05 - mae: 0.0034 - val_loss: 9.8325e-05 - val_mae: 0.0035\n",
      "Epoch 57/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 8.6761e-05 - mae: 0.0036 - val_loss: 8.9184e-05 - val_mae: 0.0036\n",
      "Epoch 58/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 7.7914e-05 - mae: 0.0033 - val_loss: 8.6915e-05 - val_mae: 0.0036\n",
      "Epoch 59/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 8.9431e-05 - mae: 0.0038 - val_loss: 9.0322e-05 - val_mae: 0.0037\n",
      "Epoch 60/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 9.0297e-05 - mae: 0.0037 - val_loss: 8.9438e-05 - val_mae: 0.0036\n",
      "Epoch 61/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 9.3034e-05 - mae: 0.0038 - val_loss: 1.1009e-04 - val_mae: 0.0043\n",
      "Epoch 62/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 7.2515e-05 - mae: 0.0032 - val_loss: 8.0797e-05 - val_mae: 0.0031\n",
      "Epoch 63/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 7.5458e-05 - mae: 0.0033 - val_loss: 8.6755e-05 - val_mae: 0.0034\n",
      "Epoch 64/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 7.5651e-05 - mae: 0.0033 - val_loss: 7.8274e-05 - val_mae: 0.0032\n",
      "Epoch 65/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 7.1675e-05 - mae: 0.0032 - val_loss: 8.5493e-05 - val_mae: 0.0034\n",
      "Epoch 66/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 7.5099e-05 - mae: 0.0033 - val_loss: 1.0427e-04 - val_mae: 0.0044\n",
      "Epoch 67/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 8.4721e-05 - mae: 0.0038 - val_loss: 7.5888e-05 - val_mae: 0.0032\n",
      "Epoch 68/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.7472e-05 - mae: 0.0030 - val_loss: 8.4022e-05 - val_mae: 0.0035\n",
      "Epoch 69/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 6.4877e-05 - mae: 0.0030 - val_loss: 8.3727e-05 - val_mae: 0.0033\n",
      "Epoch 70/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 7.4439e-05 - mae: 0.0034 - val_loss: 8.0386e-05 - val_mae: 0.0034\n",
      "Epoch 71/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 7.3756e-05 - mae: 0.0033 - val_loss: 9.2154e-05 - val_mae: 0.0040\n",
      "Epoch 72/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 7.1240e-05 - mae: 0.0033 - val_loss: 8.2839e-05 - val_mae: 0.0035\n",
      "Epoch 73/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 7.2526e-05 - mae: 0.0033 - val_loss: 7.8068e-05 - val_mae: 0.0034\n",
      "Epoch 74/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 7.6906e-05 - mae: 0.0035 - val_loss: 8.6142e-05 - val_mae: 0.0034\n",
      "Epoch 75/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 6.6470e-05 - mae: 0.0031 - val_loss: 8.4138e-05 - val_mae: 0.0035\n",
      "Epoch 76/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.3981e-05 - mae: 0.0030 - val_loss: 7.4597e-05 - val_mae: 0.0033\n",
      "Epoch 77/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.5187e-05 - mae: 0.0031 - val_loss: 6.9407e-05 - val_mae: 0.0031\n",
      "Epoch 78/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 6.6682e-05 - mae: 0.0031 - val_loss: 7.7087e-05 - val_mae: 0.0031\n",
      "Epoch 79/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.5263e-05 - mae: 0.0031 - val_loss: 8.2621e-05 - val_mae: 0.0037\n",
      "Epoch 80/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.9184e-05 - mae: 0.0028 - val_loss: 7.8754e-05 - val_mae: 0.0035\n",
      "Epoch 81/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 6.1700e-05 - mae: 0.0030 - val_loss: 8.1309e-05 - val_mae: 0.0036\n",
      "Epoch 82/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 6.7724e-05 - mae: 0.0032 - val_loss: 8.0564e-05 - val_mae: 0.0034\n",
      "Epoch 83/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 6.2991e-05 - mae: 0.0030 - val_loss: 7.9642e-05 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 6.0646e-05 - mae: 0.0030 - val_loss: 9.9392e-05 - val_mae: 0.0041\n",
      "Epoch 85/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 6.0760e-05 - mae: 0.0029 - val_loss: 7.1592e-05 - val_mae: 0.0030\n",
      "Epoch 86/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 6.5965e-05 - mae: 0.0032 - val_loss: 7.4124e-05 - val_mae: 0.0029\n",
      "Epoch 87/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 6.1680e-05 - mae: 0.0030 - val_loss: 7.4816e-05 - val_mae: 0.0033\n",
      "Epoch 88/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.0405e-05 - mae: 0.0030 - val_loss: 7.3106e-05 - val_mae: 0.0030\n",
      "Epoch 89/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 5.9257e-05 - mae: 0.0029 - val_loss: 6.2100e-05 - val_mae: 0.0026\n",
      "Epoch 90/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 6.0077e-05 - mae: 0.0030 - val_loss: 6.7376e-05 - val_mae: 0.0028\n",
      "Epoch 91/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 6.0411e-05 - mae: 0.0030 - val_loss: 6.6792e-05 - val_mae: 0.0030\n",
      "Epoch 92/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.8697e-05 - mae: 0.0029 - val_loss: 7.1107e-05 - val_mae: 0.0031\n",
      "Epoch 93/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 6.1454e-05 - mae: 0.0031 - val_loss: 7.0483e-05 - val_mae: 0.0033\n",
      "Epoch 94/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.8629e-05 - mae: 0.0029 - val_loss: 6.7643e-05 - val_mae: 0.0031\n",
      "Epoch 95/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 6.0778e-05 - mae: 0.0030 - val_loss: 8.2647e-05 - val_mae: 0.0036\n",
      "Epoch 96/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.8483e-05 - mae: 0.0030 - val_loss: 6.8450e-05 - val_mae: 0.0029\n",
      "Epoch 97/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.8544e-05 - mae: 0.0030 - val_loss: 6.3602e-05 - val_mae: 0.0028\n",
      "Epoch 98/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 6.0529e-05 - mae: 0.0029 - val_loss: 6.2841e-05 - val_mae: 0.0027\n",
      "Epoch 99/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 6.0189e-05 - mae: 0.0031 - val_loss: 7.5825e-05 - val_mae: 0.0033\n",
      "Epoch 100/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.5962e-05 - mae: 0.0029 - val_loss: 7.5002e-05 - val_mae: 0.0032\n",
      "Epoch 101/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.9426e-05 - mae: 0.0030 - val_loss: 9.5606e-05 - val_mae: 0.0038\n",
      "Epoch 102/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.1980e-05 - mae: 0.0026 - val_loss: 6.1063e-05 - val_mae: 0.0026\n",
      "Epoch 103/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 6.0263e-05 - mae: 0.0030 - val_loss: 6.4070e-05 - val_mae: 0.0028\n",
      "Epoch 104/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.7479e-05 - mae: 0.0029 - val_loss: 8.4327e-05 - val_mae: 0.0036\n",
      "Epoch 105/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.6060e-05 - mae: 0.0029 - val_loss: 6.5855e-05 - val_mae: 0.0031\n",
      "Epoch 106/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 5.1738e-05 - mae: 0.0027 - val_loss: 5.8581e-05 - val_mae: 0.0027\n",
      "Epoch 107/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.4935e-05 - mae: 0.0029 - val_loss: 7.1839e-05 - val_mae: 0.0033\n",
      "Epoch 108/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.9250e-05 - mae: 0.0030 - val_loss: 5.9636e-05 - val_mae: 0.0026\n",
      "Epoch 109/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.9998e-05 - mae: 0.0026 - val_loss: 6.9199e-05 - val_mae: 0.0031\n",
      "Epoch 110/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.3236e-05 - mae: 0.0028 - val_loss: 5.9437e-05 - val_mae: 0.0025\n",
      "Epoch 111/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.7911e-05 - mae: 0.0030 - val_loss: 7.8435e-05 - val_mae: 0.0034\n",
      "Epoch 112/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.9607e-05 - mae: 0.0026 - val_loss: 6.6859e-05 - val_mae: 0.0034\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 16s 142ms/step - loss: 5.7049e-05 - mae: 0.0030 - val_loss: 7.0060e-05 - val_mae: 0.0032\n",
      "Epoch 114/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 5.1417e-05 - mae: 0.0027 - val_loss: 7.0800e-05 - val_mae: 0.0033\n",
      "Epoch 115/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 6.0992e-05 - mae: 0.0030 - val_loss: 1.4356e-04 - val_mae: 0.0051\n",
      "Epoch 116/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 6.4520e-05 - mae: 0.0032 - val_loss: 6.5206e-05 - val_mae: 0.0031\n",
      "Epoch 117/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.9702e-05 - mae: 0.0026 - val_loss: 6.1064e-05 - val_mae: 0.0027\n",
      "Epoch 118/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.2330e-05 - mae: 0.0028 - val_loss: 6.4438e-05 - val_mae: 0.0031\n",
      "Epoch 119/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.1707e-05 - mae: 0.0027 - val_loss: 5.6812e-05 - val_mae: 0.0028\n",
      "Epoch 120/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.9381e-05 - mae: 0.0031 - val_loss: 8.6419e-05 - val_mae: 0.0039\n",
      "Epoch 121/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.3433e-05 - mae: 0.0028 - val_loss: 7.2319e-05 - val_mae: 0.0034\n",
      "Epoch 122/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 5.3199e-05 - mae: 0.0028 - val_loss: 7.3382e-05 - val_mae: 0.0034\n",
      "Epoch 123/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.2251e-05 - mae: 0.0027 - val_loss: 6.1858e-05 - val_mae: 0.0027\n",
      "Epoch 124/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.1323e-05 - mae: 0.0027 - val_loss: 1.2841e-04 - val_mae: 0.0051\n",
      "Epoch 125/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 7.4479e-05 - mae: 0.0036 - val_loss: 7.7339e-05 - val_mae: 0.0035\n",
      "Epoch 126/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.0419e-05 - mae: 0.0026 - val_loss: 6.9191e-05 - val_mae: 0.0034\n",
      "Epoch 127/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.7373e-05 - mae: 0.0025 - val_loss: 5.8092e-05 - val_mae: 0.0026\n",
      "Epoch 128/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.7065e-05 - mae: 0.0025 - val_loss: 5.5284e-05 - val_mae: 0.0025\n",
      "Epoch 129/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.9722e-05 - mae: 0.0027 - val_loss: 5.8031e-05 - val_mae: 0.0025\n",
      "Epoch 130/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.9445e-05 - mae: 0.0026 - val_loss: 5.4547e-05 - val_mae: 0.0024\n",
      "Epoch 131/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.6717e-05 - mae: 0.0025 - val_loss: 5.9883e-05 - val_mae: 0.0027\n",
      "Epoch 132/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.8257e-05 - mae: 0.0026 - val_loss: 6.5699e-05 - val_mae: 0.0031\n",
      "Epoch 133/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 4.8600e-05 - mae: 0.0026 - val_loss: 7.0707e-05 - val_mae: 0.0032\n",
      "Epoch 134/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.0303e-05 - mae: 0.0027 - val_loss: 6.5965e-05 - val_mae: 0.0033\n",
      "Epoch 135/1000\n",
      "116/116 [==============================] - 16s 138ms/step - loss: 5.5459e-05 - mae: 0.0029 - val_loss: 7.2868e-05 - val_mae: 0.0035\n",
      "Epoch 136/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.3368e-05 - mae: 0.0029 - val_loss: 6.7793e-05 - val_mae: 0.0034\n",
      "Epoch 137/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 5.0207e-05 - mae: 0.0027 - val_loss: 7.0868e-05 - val_mae: 0.0033\n",
      "Epoch 138/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.4600e-05 - mae: 0.0029 - val_loss: 5.9740e-05 - val_mae: 0.0028\n",
      "Epoch 139/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.1002e-05 - mae: 0.0028 - val_loss: 7.5197e-05 - val_mae: 0.0035\n",
      "Epoch 140/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.9308e-05 - mae: 0.0027 - val_loss: 5.8505e-05 - val_mae: 0.0028\n",
      "Epoch 141/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.7863e-05 - mae: 0.0026 - val_loss: 5.9674e-05 - val_mae: 0.0026\n",
      "Epoch 142/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 4.7541e-05 - mae: 0.0026 - val_loss: 5.6215e-05 - val_mae: 0.0024\n",
      "Epoch 143/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.0217e-05 - mae: 0.0027 - val_loss: 5.9753e-05 - val_mae: 0.0029\n",
      "Epoch 144/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 3.8101e-04 - mae: 0.0072 - val_loss: 3.0268e-04 - val_mae: 0.0078\n",
      "Epoch 145/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 1.9157e-04 - mae: 0.0058 - val_loss: 1.8662e-04 - val_mae: 0.0051\n",
      "Epoch 146/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 1.4656e-04 - mae: 0.0049 - val_loss: 1.5354e-04 - val_mae: 0.0047\n",
      "Epoch 147/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 1.1785e-04 - mae: 0.0044 - val_loss: 1.1718e-04 - val_mae: 0.0042\n",
      "Epoch 148/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 8.9971e-05 - mae: 0.0037 - val_loss: 1.0322e-04 - val_mae: 0.0042\n",
      "Epoch 149/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 8.0611e-05 - mae: 0.0035 - val_loss: 1.0813e-04 - val_mae: 0.0041\n",
      "Epoch 150/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 8.0304e-05 - mae: 0.0036 - val_loss: 9.2830e-05 - val_mae: 0.0038\n",
      "Epoch 151/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 7.2473e-05 - mae: 0.0033 - val_loss: 8.3242e-05 - val_mae: 0.0031\n",
      "Epoch 152/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 6.6995e-05 - mae: 0.0030 - val_loss: 7.6303e-05 - val_mae: 0.0031\n",
      "Epoch 153/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 6.7343e-05 - mae: 0.0031 - val_loss: 7.6836e-05 - val_mae: 0.0030\n",
      "Epoch 154/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 6.5958e-05 - mae: 0.0030 - val_loss: 8.6145e-05 - val_mae: 0.0037\n",
      "Epoch 155/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.5210e-05 - mae: 0.0030 - val_loss: 7.4963e-05 - val_mae: 0.0031\n",
      "Epoch 156/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 6.4464e-05 - mae: 0.0030 - val_loss: 7.5039e-05 - val_mae: 0.0033\n",
      "Epoch 157/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 6.5965e-05 - mae: 0.0031 - val_loss: 7.0846e-05 - val_mae: 0.0029\n",
      "Epoch 158/1000\n",
      "116/116 [==============================] - 16s 139ms/step - loss: 6.4126e-05 - mae: 0.0031 - val_loss: 9.5813e-05 - val_mae: 0.0038\n",
      "Epoch 159/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 6.6930e-05 - mae: 0.0032 - val_loss: 8.6672e-05 - val_mae: 0.0038\n",
      "Epoch 160/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 6.3028e-05 - mae: 0.0030 - val_loss: 7.7258e-05 - val_mae: 0.0032\n",
      "Epoch 161/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 6.2369e-05 - mae: 0.0031 - val_loss: 1.1914e-04 - val_mae: 0.0051\n",
      "Epoch 162/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 6.6798e-05 - mae: 0.0033 - val_loss: 7.3566e-05 - val_mae: 0.0031\n",
      "Epoch 163/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.8966e-05 - mae: 0.0029 - val_loss: 8.0460e-05 - val_mae: 0.0036\n",
      "Epoch 164/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 6.0611e-05 - mae: 0.0030 - val_loss: 6.8069e-05 - val_mae: 0.0029\n",
      "Epoch 165/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.7751e-05 - mae: 0.0028 - val_loss: 6.7860e-05 - val_mae: 0.0028\n",
      "Epoch 166/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 6.4485e-05 - mae: 0.0033 - val_loss: 7.0992e-05 - val_mae: 0.0034\n",
      "Epoch 167/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 6.5252e-05 - mae: 0.0032 - val_loss: 1.0251e-04 - val_mae: 0.0051\n",
      "Epoch 168/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 6.4975e-05 - mae: 0.0032 - val_loss: 6.9394e-05 - val_mae: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.8084e-05 - mae: 0.0030 - val_loss: 6.5414e-05 - val_mae: 0.0029\n",
      "Epoch 170/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 5.3914e-05 - mae: 0.0027 - val_loss: 7.1059e-05 - val_mae: 0.0030\n",
      "Epoch 171/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 6.0457e-05 - mae: 0.0031 - val_loss: 6.1493e-05 - val_mae: 0.0026\n",
      "Epoch 172/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.7303e-05 - mae: 0.0029 - val_loss: 6.6400e-05 - val_mae: 0.0030\n",
      "Epoch 173/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.6105e-05 - mae: 0.0029 - val_loss: 8.6536e-05 - val_mae: 0.0038\n",
      "Epoch 174/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 6.1712e-05 - mae: 0.0032 - val_loss: 6.6812e-05 - val_mae: 0.0030\n",
      "Epoch 175/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.6297e-05 - mae: 0.0030 - val_loss: 7.0278e-05 - val_mae: 0.0030\n",
      "Epoch 176/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.3183e-05 - mae: 0.0027 - val_loss: 6.1144e-05 - val_mae: 0.0025\n",
      "Epoch 177/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 5.5632e-05 - mae: 0.0028 - val_loss: 6.3003e-05 - val_mae: 0.0027\n",
      "Epoch 178/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.8739e-05 - mae: 0.0031 - val_loss: 6.9809e-05 - val_mae: 0.0033\n",
      "Epoch 179/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 5.4533e-05 - mae: 0.0029 - val_loss: 6.1467e-05 - val_mae: 0.0028\n",
      "Epoch 180/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.3731e-05 - mae: 0.0029 - val_loss: 6.1724e-05 - val_mae: 0.0029\n",
      "Epoch 181/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.0569e-05 - mae: 0.0027 - val_loss: 6.1623e-05 - val_mae: 0.0031\n",
      "Epoch 182/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.5517e-05 - mae: 0.0029 - val_loss: 6.2585e-05 - val_mae: 0.0029\n",
      "Epoch 183/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.6398e-05 - mae: 0.0030 - val_loss: 5.6990e-05 - val_mae: 0.0025\n",
      "Epoch 184/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 4.8951e-05 - mae: 0.0026 - val_loss: 6.0412e-05 - val_mae: 0.0028\n",
      "Epoch 185/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.0377e-05 - mae: 0.0027 - val_loss: 7.9214e-05 - val_mae: 0.0039\n",
      "Epoch 186/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.4526e-05 - mae: 0.0029 - val_loss: 6.9457e-05 - val_mae: 0.0035\n",
      "Epoch 187/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 5.1262e-05 - mae: 0.0027 - val_loss: 6.0020e-05 - val_mae: 0.0028\n",
      "Epoch 188/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.4338e-05 - mae: 0.0030 - val_loss: 5.8997e-05 - val_mae: 0.0026\n",
      "Epoch 189/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 7.3239e-05 - mae: 0.0036 - val_loss: 1.0778e-04 - val_mae: 0.0050\n",
      "Epoch 190/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.7582e-05 - mae: 0.0030 - val_loss: 6.0681e-05 - val_mae: 0.0029\n",
      "Epoch 191/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.0917e-05 - mae: 0.0028 - val_loss: 5.7424e-05 - val_mae: 0.0025\n",
      "Epoch 192/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.9189e-05 - mae: 0.0026 - val_loss: 6.4233e-05 - val_mae: 0.0030\n",
      "Epoch 193/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 5.1719e-05 - mae: 0.0028 - val_loss: 1.1264e-04 - val_mae: 0.0047\n",
      "Epoch 194/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 5.6303e-05 - mae: 0.0030 - val_loss: 5.8591e-05 - val_mae: 0.0026\n",
      "Epoch 195/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.5255e-05 - mae: 0.0029 - val_loss: 5.9445e-05 - val_mae: 0.0025\n",
      "Epoch 196/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.8549e-05 - mae: 0.0026 - val_loss: 5.7503e-05 - val_mae: 0.0025\n",
      "Epoch 197/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.2658e-05 - mae: 0.0029 - val_loss: 6.8137e-05 - val_mae: 0.0033\n",
      "Epoch 198/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.8169e-05 - mae: 0.0026 - val_loss: 6.0710e-05 - val_mae: 0.0031\n",
      "Epoch 199/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.2920e-05 - mae: 0.0029 - val_loss: 9.9699e-05 - val_mae: 0.0047\n",
      "Epoch 200/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.4043e-05 - mae: 0.0030 - val_loss: 6.8122e-05 - val_mae: 0.0033\n",
      "Epoch 201/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.2696e-05 - mae: 0.0029 - val_loss: 6.1443e-05 - val_mae: 0.0031\n",
      "Epoch 202/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.9603e-05 - mae: 0.0031 - val_loss: 6.1093e-05 - val_mae: 0.0027\n",
      "Epoch 203/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.9667e-05 - mae: 0.0027 - val_loss: 6.0888e-05 - val_mae: 0.0030\n",
      "Epoch 204/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.7346e-05 - mae: 0.0030 - val_loss: 6.4559e-05 - val_mae: 0.0033\n",
      "Epoch 205/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 5.2308e-05 - mae: 0.0029 - val_loss: 6.9429e-05 - val_mae: 0.0034\n",
      "Epoch 206/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.2128e-05 - mae: 0.0028 - val_loss: 5.7767e-05 - val_mae: 0.0028\n",
      "Epoch 207/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 5.1578e-05 - mae: 0.0028 - val_loss: 6.8328e-05 - val_mae: 0.0034\n",
      "Epoch 208/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.1967e-05 - mae: 0.0028 - val_loss: 5.6604e-05 - val_mae: 0.0027\n",
      "Epoch 209/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.9992e-05 - mae: 0.0028 - val_loss: 5.9032e-05 - val_mae: 0.0027\n",
      "Epoch 210/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 5.0858e-05 - mae: 0.0028 - val_loss: 5.6729e-05 - val_mae: 0.0028\n",
      "Epoch 211/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.6976e-05 - mae: 0.0026 - val_loss: 7.7636e-05 - val_mae: 0.0037\n",
      "Epoch 212/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.3971e-05 - mae: 0.0029 - val_loss: 5.7308e-05 - val_mae: 0.0029\n",
      "Epoch 213/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.7984e-05 - mae: 0.0026 - val_loss: 7.3754e-05 - val_mae: 0.0033\n",
      "Epoch 214/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.8885e-05 - mae: 0.0026 - val_loss: 5.9536e-05 - val_mae: 0.0028\n",
      "Epoch 215/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.4145e-05 - mae: 0.0029 - val_loss: 6.7055e-05 - val_mae: 0.0032\n",
      "Epoch 216/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 5.0940e-05 - mae: 0.0028 - val_loss: 6.7092e-05 - val_mae: 0.0032\n",
      "Epoch 217/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.0807e-05 - mae: 0.0028 - val_loss: 6.4997e-05 - val_mae: 0.0035\n",
      "Epoch 218/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.9276e-05 - mae: 0.0027 - val_loss: 5.6482e-05 - val_mae: 0.0026\n",
      "Epoch 219/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.5732e-05 - mae: 0.0025 - val_loss: 6.0058e-05 - val_mae: 0.0028\n",
      "Epoch 220/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.7756e-05 - mae: 0.0026 - val_loss: 5.8106e-05 - val_mae: 0.0027\n",
      "Epoch 221/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.3454e-05 - mae: 0.0029 - val_loss: 6.2834e-05 - val_mae: 0.0031\n",
      "Epoch 222/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.1811e-05 - mae: 0.0029 - val_loss: 5.8181e-05 - val_mae: 0.0028\n",
      "Epoch 223/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.7350e-05 - mae: 0.0029 - val_loss: 5.1174e-05 - val_mae: 0.0022\n",
      "Epoch 224/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.2385e-05 - mae: 0.0028 - val_loss: 5.1400e-05 - val_mae: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.0023e-05 - mae: 0.0027 - val_loss: 5.3405e-05 - val_mae: 0.0022\n",
      "Epoch 226/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.6332e-05 - mae: 0.0025 - val_loss: 6.4839e-05 - val_mae: 0.0030\n",
      "Epoch 227/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.8126e-05 - mae: 0.0030 - val_loss: 5.7315e-05 - val_mae: 0.0027\n",
      "Epoch 228/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.7968e-05 - mae: 0.0026 - val_loss: 6.4354e-05 - val_mae: 0.0031\n",
      "Epoch 229/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.8291e-05 - mae: 0.0027 - val_loss: 5.5527e-05 - val_mae: 0.0027\n",
      "Epoch 230/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.6498e-05 - mae: 0.0026 - val_loss: 5.5868e-05 - val_mae: 0.0026\n",
      "Epoch 231/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.8781e-05 - mae: 0.0027 - val_loss: 5.6542e-05 - val_mae: 0.0027\n",
      "Epoch 232/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.3277e-05 - mae: 0.0029 - val_loss: 6.5426e-05 - val_mae: 0.0031\n",
      "Epoch 233/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 5.0420e-05 - mae: 0.0028 - val_loss: 7.0656e-05 - val_mae: 0.0032\n",
      "Epoch 234/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.3327e-05 - mae: 0.0029 - val_loss: 8.4492e-05 - val_mae: 0.0039\n",
      "Epoch 235/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 5.1793e-05 - mae: 0.0028 - val_loss: 6.3084e-05 - val_mae: 0.0031\n",
      "Epoch 236/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.5626e-05 - mae: 0.0025 - val_loss: 6.1277e-05 - val_mae: 0.0029\n",
      "Epoch 237/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.8951e-05 - mae: 0.0027 - val_loss: 6.2721e-05 - val_mae: 0.0035\n",
      "Epoch 238/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.9548e-05 - mae: 0.0027 - val_loss: 8.1340e-05 - val_mae: 0.0037\n",
      "Epoch 239/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.9676e-05 - mae: 0.0027 - val_loss: 5.8275e-05 - val_mae: 0.0030\n",
      "Epoch 240/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.5536e-05 - mae: 0.0025 - val_loss: 6.3051e-05 - val_mae: 0.0031\n",
      "Epoch 241/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.8082e-05 - mae: 0.0026 - val_loss: 5.9891e-05 - val_mae: 0.0028\n",
      "Epoch 242/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 5.7972e-05 - mae: 0.0031 - val_loss: 6.4363e-05 - val_mae: 0.0030\n",
      "Epoch 243/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.0003e-05 - mae: 0.0028 - val_loss: 5.6740e-05 - val_mae: 0.0028\n",
      "Epoch 244/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5176e-05 - mae: 0.0025 - val_loss: 5.4399e-05 - val_mae: 0.0025\n",
      "Epoch 245/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.8801e-05 - mae: 0.0026 - val_loss: 8.4310e-05 - val_mae: 0.0041\n",
      "Epoch 246/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 5.4919e-05 - mae: 0.0029 - val_loss: 6.3334e-05 - val_mae: 0.0032\n",
      "Epoch 247/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.8976e-05 - mae: 0.0027 - val_loss: 6.0461e-05 - val_mae: 0.0032\n",
      "Epoch 248/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.5576e-05 - mae: 0.0025 - val_loss: 7.2264e-05 - val_mae: 0.0033\n",
      "Epoch 249/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 5.1910e-05 - mae: 0.0029 - val_loss: 6.8018e-05 - val_mae: 0.0030\n",
      "Epoch 250/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.8756e-05 - mae: 0.0026 - val_loss: 5.6745e-05 - val_mae: 0.0026\n",
      "Epoch 251/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.5355e-05 - mae: 0.0025 - val_loss: 5.8104e-05 - val_mae: 0.0029\n",
      "Epoch 252/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.6229e-05 - mae: 0.0025 - val_loss: 5.9266e-05 - val_mae: 0.0029\n",
      "Epoch 253/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 5.2161e-05 - mae: 0.0028 - val_loss: 6.7536e-05 - val_mae: 0.0032\n",
      "Epoch 254/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5514e-05 - mae: 0.0025 - val_loss: 5.3890e-05 - val_mae: 0.0025\n",
      "Epoch 255/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 5.3430e-05 - mae: 0.0029 - val_loss: 5.7910e-05 - val_mae: 0.0026\n",
      "Epoch 256/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 4.6312e-05 - mae: 0.0026 - val_loss: 5.5119e-05 - val_mae: 0.0027\n",
      "Epoch 257/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.9702e-05 - mae: 0.0028 - val_loss: 5.6051e-05 - val_mae: 0.0028\n",
      "Epoch 258/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.8328e-05 - mae: 0.0027 - val_loss: 4.9094e-05 - val_mae: 0.0021\n",
      "Epoch 259/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.5429e-05 - mae: 0.0025 - val_loss: 5.1673e-05 - val_mae: 0.0023\n",
      "Epoch 260/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.9453e-05 - mae: 0.0027 - val_loss: 6.4938e-05 - val_mae: 0.0034\n",
      "Epoch 261/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.9849e-05 - mae: 0.0027 - val_loss: 5.2073e-05 - val_mae: 0.0024\n",
      "Epoch 262/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.6319e-05 - mae: 0.0025 - val_loss: 5.4689e-05 - val_mae: 0.0029\n",
      "Epoch 263/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.4483e-05 - mae: 0.0024 - val_loss: 5.2797e-05 - val_mae: 0.0025\n",
      "Epoch 264/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.6121e-05 - mae: 0.0026 - val_loss: 5.4369e-05 - val_mae: 0.0027\n",
      "Epoch 265/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.5841e-05 - mae: 0.0026 - val_loss: 5.2174e-05 - val_mae: 0.0024\n",
      "Epoch 266/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.6689e-05 - mae: 0.0026 - val_loss: 5.2991e-05 - val_mae: 0.0026\n",
      "Epoch 267/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.5056e-05 - mae: 0.0025 - val_loss: 5.3792e-05 - val_mae: 0.0025\n",
      "Epoch 268/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.8002e-05 - mae: 0.0027 - val_loss: 5.8548e-05 - val_mae: 0.0029\n",
      "Epoch 269/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 5.1909e-05 - mae: 0.0028 - val_loss: 5.9694e-05 - val_mae: 0.0027\n",
      "Epoch 270/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.5106e-05 - mae: 0.0025 - val_loss: 5.4139e-05 - val_mae: 0.0027\n",
      "Epoch 271/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.2802e-05 - mae: 0.0023 - val_loss: 5.6650e-05 - val_mae: 0.0028\n",
      "Epoch 272/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.9851e-05 - mae: 0.0027 - val_loss: 7.3328e-05 - val_mae: 0.0038\n",
      "Epoch 273/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5906e-05 - mae: 0.0026 - val_loss: 5.5208e-05 - val_mae: 0.0027\n",
      "Epoch 274/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.4705e-05 - mae: 0.0025 - val_loss: 5.1308e-05 - val_mae: 0.0025\n",
      "Epoch 275/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.3050e-05 - mae: 0.0024 - val_loss: 5.2562e-05 - val_mae: 0.0025\n",
      "Epoch 276/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.4985e-05 - mae: 0.0025 - val_loss: 5.2027e-05 - val_mae: 0.0024\n",
      "Epoch 277/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.7318e-05 - mae: 0.0026 - val_loss: 6.7441e-05 - val_mae: 0.0036\n",
      "Epoch 278/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.6642e-05 - mae: 0.0026 - val_loss: 5.8551e-05 - val_mae: 0.0028\n",
      "Epoch 279/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5529e-05 - mae: 0.0025 - val_loss: 6.0273e-05 - val_mae: 0.0028\n",
      "Epoch 280/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 5.3787e-05 - mae: 0.0030 - val_loss: 5.3670e-05 - val_mae: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2367e-05 - mae: 0.0023 - val_loss: 5.2948e-05 - val_mae: 0.0027\n",
      "Epoch 282/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.2936e-05 - mae: 0.0024 - val_loss: 6.8119e-05 - val_mae: 0.0034\n",
      "Epoch 283/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.6307e-05 - mae: 0.0026 - val_loss: 6.8442e-05 - val_mae: 0.0033\n",
      "Epoch 284/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.4131e-05 - mae: 0.0025 - val_loss: 5.3847e-05 - val_mae: 0.0025\n",
      "Epoch 285/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.7642e-05 - mae: 0.0027 - val_loss: 6.6877e-05 - val_mae: 0.0032\n",
      "Epoch 286/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.5975e-05 - mae: 0.0026 - val_loss: 5.2608e-05 - val_mae: 0.0024\n",
      "Epoch 287/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.4287e-05 - mae: 0.0024 - val_loss: 5.1128e-05 - val_mae: 0.0024\n",
      "Epoch 288/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.2943e-05 - mae: 0.0024 - val_loss: 5.1037e-05 - val_mae: 0.0023\n",
      "Epoch 289/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.5859e-05 - mae: 0.0026 - val_loss: 6.6040e-05 - val_mae: 0.0029\n",
      "Epoch 290/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.6667e-05 - mae: 0.0026 - val_loss: 5.2003e-05 - val_mae: 0.0024\n",
      "Epoch 291/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.8068e-05 - mae: 0.0027 - val_loss: 4.9703e-05 - val_mae: 0.0023\n",
      "Epoch 292/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.2859e-05 - mae: 0.0024 - val_loss: 5.0276e-05 - val_mae: 0.0023\n",
      "Epoch 293/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.2119e-05 - mae: 0.0023 - val_loss: 5.3557e-05 - val_mae: 0.0026\n",
      "Epoch 294/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5411e-05 - mae: 0.0026 - val_loss: 5.3222e-05 - val_mae: 0.0025\n",
      "Epoch 295/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.9277e-05 - mae: 0.0026 - val_loss: 5.6764e-05 - val_mae: 0.0027\n",
      "Epoch 296/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.5172e-05 - mae: 0.0025 - val_loss: 5.1063e-05 - val_mae: 0.0024\n",
      "Epoch 297/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.5222e-05 - mae: 0.0025 - val_loss: 5.6528e-05 - val_mae: 0.0027\n",
      "Epoch 298/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 6.1659e-05 - mae: 0.0030 - val_loss: 6.1591e-05 - val_mae: 0.0034\n",
      "Epoch 299/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.6017e-05 - mae: 0.0026 - val_loss: 5.5453e-05 - val_mae: 0.0026\n",
      "Epoch 300/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.5940e-05 - mae: 0.0025 - val_loss: 5.3702e-05 - val_mae: 0.0027\n",
      "Epoch 301/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.4109e-05 - mae: 0.0025 - val_loss: 5.1036e-05 - val_mae: 0.0024\n",
      "Epoch 302/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.7221e-05 - mae: 0.0027 - val_loss: 7.1337e-05 - val_mae: 0.0038\n",
      "Epoch 303/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.5124e-05 - mae: 0.0025 - val_loss: 5.2820e-05 - val_mae: 0.0023\n",
      "Epoch 304/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.3594e-05 - mae: 0.0024 - val_loss: 5.7202e-05 - val_mae: 0.0027\n",
      "Epoch 305/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.4968e-05 - mae: 0.0025 - val_loss: 6.3628e-05 - val_mae: 0.0031\n",
      "Epoch 306/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.7043e-05 - mae: 0.0026 - val_loss: 6.5693e-05 - val_mae: 0.0036\n",
      "Epoch 307/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.5580e-05 - mae: 0.0026 - val_loss: 7.2775e-05 - val_mae: 0.0034\n",
      "Epoch 308/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.8966e-05 - mae: 0.0027 - val_loss: 4.8934e-05 - val_mae: 0.0021\n",
      "Epoch 309/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.3414e-05 - mae: 0.0024 - val_loss: 5.2226e-05 - val_mae: 0.0026\n",
      "Epoch 310/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.4220e-05 - mae: 0.0025 - val_loss: 5.2259e-05 - val_mae: 0.0024\n",
      "Epoch 311/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.4215e-05 - mae: 0.0025 - val_loss: 6.0776e-05 - val_mae: 0.0031\n",
      "Epoch 312/1000\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 4.4271e-05 - mae: 0.0025 - val_loss: 5.5540e-05 - val_mae: 0.0029\n",
      "Epoch 313/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2448e-05 - mae: 0.0024 - val_loss: 5.4300e-05 - val_mae: 0.0024\n",
      "Epoch 314/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.3970e-05 - mae: 0.0025 - val_loss: 4.7616e-05 - val_mae: 0.0023\n",
      "Epoch 315/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.2284e-05 - mae: 0.0024 - val_loss: 6.4322e-05 - val_mae: 0.0030\n",
      "Epoch 316/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.6045e-05 - mae: 0.0026 - val_loss: 5.0227e-05 - val_mae: 0.0023\n",
      "Epoch 317/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.4114e-05 - mae: 0.0025 - val_loss: 6.1319e-05 - val_mae: 0.0033\n",
      "Epoch 318/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.4525e-05 - mae: 0.0026 - val_loss: 6.2862e-05 - val_mae: 0.0033\n",
      "Epoch 319/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.3838e-05 - mae: 0.0025 - val_loss: 5.6144e-05 - val_mae: 0.0028\n",
      "Epoch 320/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.2977e-05 - mae: 0.0024 - val_loss: 6.8502e-05 - val_mae: 0.0035\n",
      "Epoch 321/1000\n",
      "116/116 [==============================] - 16s 141ms/step - loss: 4.6847e-05 - mae: 0.0026 - val_loss: 5.2361e-05 - val_mae: 0.0025\n",
      "Epoch 322/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.3875e-05 - mae: 0.0024 - val_loss: 5.6385e-05 - val_mae: 0.0027\n",
      "Epoch 323/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.8002e-05 - mae: 0.0028 - val_loss: 5.3806e-05 - val_mae: 0.0026\n",
      "Epoch 324/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.4899e-05 - mae: 0.0025 - val_loss: 5.0682e-05 - val_mae: 0.0024\n",
      "Epoch 325/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.3738e-05 - mae: 0.0024 - val_loss: 6.6552e-05 - val_mae: 0.0034\n",
      "Epoch 326/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.5471e-05 - mae: 0.0025 - val_loss: 4.8057e-05 - val_mae: 0.0022\n",
      "Epoch 327/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.9064e-05 - mae: 0.0028 - val_loss: 5.2201e-05 - val_mae: 0.0024\n",
      "Epoch 328/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.3812e-05 - mae: 0.0024 - val_loss: 5.4535e-05 - val_mae: 0.0026\n",
      "Epoch 329/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.4797e-05 - mae: 0.0025 - val_loss: 5.2403e-05 - val_mae: 0.0027\n",
      "Epoch 330/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.3461e-05 - mae: 0.0024 - val_loss: 5.4648e-05 - val_mae: 0.0027\n",
      "Epoch 331/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.4008e-05 - mae: 0.0025 - val_loss: 5.3278e-05 - val_mae: 0.0026\n",
      "Epoch 332/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2934e-05 - mae: 0.0025 - val_loss: 7.3185e-05 - val_mae: 0.0040\n",
      "Epoch 333/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2335e-05 - mae: 0.0024 - val_loss: 5.2561e-05 - val_mae: 0.0025\n",
      "Epoch 334/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.4407e-05 - mae: 0.0026 - val_loss: 5.0274e-05 - val_mae: 0.0026\n",
      "Epoch 335/1000\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 4.0789e-05 - mae: 0.0023 - val_loss: 4.9752e-05 - val_mae: 0.0023\n",
      "Epoch 336/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.1710e-05 - mae: 0.0024 - val_loss: 5.9224e-05 - val_mae: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.3794e-05 - mae: 0.0025 - val_loss: 4.8674e-05 - val_mae: 0.0023\n",
      "Epoch 338/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.1735e-05 - mae: 0.0024 - val_loss: 5.8282e-05 - val_mae: 0.0030\n",
      "Epoch 339/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.1614e-05 - mae: 0.0024 - val_loss: 4.9245e-05 - val_mae: 0.0023\n",
      "Epoch 340/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 3.9317e-05 - mae: 0.0022 - val_loss: 4.6532e-05 - val_mae: 0.0022\n",
      "Epoch 341/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.2143e-05 - mae: 0.0024 - val_loss: 5.0435e-05 - val_mae: 0.0026\n",
      "Epoch 342/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2211e-05 - mae: 0.0024 - val_loss: 5.2228e-05 - val_mae: 0.0024\n",
      "Epoch 343/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.3060e-05 - mae: 0.0025 - val_loss: 5.1086e-05 - val_mae: 0.0026\n",
      "Epoch 344/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.3293e-05 - mae: 0.0025 - val_loss: 4.8459e-05 - val_mae: 0.0024\n",
      "Epoch 345/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.4173e-05 - mae: 0.0026 - val_loss: 6.5095e-05 - val_mae: 0.0033\n",
      "Epoch 346/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.5334e-05 - mae: 0.0027 - val_loss: 6.6330e-05 - val_mae: 0.0035\n",
      "Epoch 347/1000\n",
      "116/116 [==============================] - 16s 140ms/step - loss: 4.5200e-05 - mae: 0.0026 - val_loss: 4.8225e-05 - val_mae: 0.0023\n",
      "Epoch 348/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.1985e-05 - mae: 0.0024 - val_loss: 4.8228e-05 - val_mae: 0.0023\n",
      "Epoch 349/1000\n",
      "116/116 [==============================] - 17s 142ms/step - loss: 4.0164e-05 - mae: 0.0023 - val_loss: 4.8835e-05 - val_mae: 0.0024\n",
      "Epoch 350/1000\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 4.2064e-05 - mae: 0.0024 - val_loss: 5.0977e-05 - val_mae: 0.0027\n",
      "Epoch 351/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.2216e-05 - mae: 0.0024 - val_loss: 5.0123e-05 - val_mae: 0.0024\n",
      "Epoch 352/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.7564e-05 - mae: 0.0027 - val_loss: 6.4020e-05 - val_mae: 0.0036\n",
      "Epoch 353/1000\n",
      "116/116 [==============================] - 17s 146ms/step - loss: 4.4337e-05 - mae: 0.0026 - val_loss: 4.7903e-05 - val_mae: 0.0023\n",
      "Epoch 354/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 4.0478e-05 - mae: 0.0023 - val_loss: 5.2032e-05 - val_mae: 0.0027\n",
      "Epoch 355/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.1351e-05 - mae: 0.0024 - val_loss: 5.0839e-05 - val_mae: 0.0024\n",
      "Epoch 356/1000\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 4.6027e-05 - mae: 0.0026 - val_loss: 5.2442e-05 - val_mae: 0.0026\n",
      "Epoch 357/1000\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 4.3171e-05 - mae: 0.0025 - val_loss: 5.8487e-05 - val_mae: 0.0034\n",
      "Epoch 358/1000\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 3.9944e-05 - mae: 0.0023 - val_loss: 4.9493e-05 - val_mae: 0.0023\n"
     ]
    }
   ],
   "source": [
    "history = model_RNN_LSTM_3d.fit(x_train,y_train, epochs=1000, batch_size=16, validation_split=0.2,callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670470311929,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "4kEwHgcTThyk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_RNN_LSTM_3d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_LSTM.save(\"/content/drive/MyDrive/Robot perception /Modeling/models/lstm_data_trim.h5\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model_LSTM.save(\"/content/drive/MyDrive/Robot perception /Modeling/models/lstm.h5\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_RNN_LSTM_3d\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/sep_1/lstm.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_RNN_LSTM_3d' is not defined"
     ]
    }
   ],
   "source": [
    "# model_LSTM.save(\"/content/drive/MyDrive/Robot perception /Modeling/models/lstm_data_trim.h5\")\n",
    "# model_LSTM.save(\"/content/drive/MyDrive/Robot perception /Modeling/models/lstm.h5\")\n",
    "model_RNN_LSTM_3d.save(\"./models/traj_point/lstm_rnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1670464132234,
     "user": {
      "displayName": "Niegil Francis Muttath Joseph",
      "userId": "14540288352466598089"
     },
     "user_tz": 300
    },
    "id": "_lI1n5rWk7DF",
    "outputId": "11fbc446-b78b-4739-c7b0-cdfd12b12134"
   },
   "outputs": [],
   "source": [
    "# classification models\n",
    "# model_load = load_model('/content/drive/MyDrive/Robot perception /Modeling/models/lstm_data_trim.h5')\n",
    "# model_load = load_model('/content/drive/MyDrive/Robot perception /Modeling/models/lstm.h5')\n",
    "model_load = load_model('./models/sep_0.7/lstm_rnn.h5')\n",
    "\n",
    "\n",
    "# 3d models\n",
    "# model_load = load_model('./models/traj_point/lstm_rnn.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 200, 3)]          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200, 15)           60        \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 200, 15)           465       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 15)           1860      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200, 9)            144       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,529\n",
      "Trainable params: 2,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_load.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 18ms/step - loss: 0.2631 - accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26308029890060425, 0.900730311870575]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2674 - accuracy: 0.8985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.267398864030838, 0.898490846157074]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the lengths of x_test\n",
    "# lengths = np.array([len(i) for i in x_test_])\n",
    "# Removing trajectories with <=12 length\n",
    "# idx = np.where(lengths <= 12)[0]\n",
    "# [y_test_.pop(j-i) for i,j in enumerate(idx)]\n",
    "# [x_test_.pop(j-i) for i,j in enumerate(idx)];\n",
    "# Checking\n",
    "# np.where(lengths <= 12)[0]\n",
    "\n",
    "# dict_l = defaultdict(list)\n",
    "\n",
    "# for i in tqdm.tqdm(range(max(lengths)-9)):\n",
    "#     idx = np.where(lengths >= i+12)[0]\n",
    "#     x_ = [x_test_[l].values.tolist()[:i+2] for l in idx]\n",
    "#     y_ = [y_test_[l].values.tolist()[i+2:i+12] for l in idx]\n",
    "#     if len(idx) == 1:\n",
    "#             break\n",
    "#     for k in range(10):\n",
    "\n",
    "#         temp = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(x_, padding='post', dtype='float', maxlen=200, value = -10))\n",
    "\n",
    "#         preds = model_load.predict(temp, verbose = 0)\n",
    "#         preds = preds[:,-1,:]\n",
    "#         [x_[j].append(list(preds[j])) for j in range(len(idx))]\n",
    "#     print(np.array(x_[j])[i+2:] - np.array(y_[j]))\n",
    "#     print(np.array(x_[j]))\n",
    "#     print(np.array(y_[j]))\n",
    "#     helo\n",
    "#     [dict_l[i+2].append(np.mean(np.linalg.norm(),axis =0)) for j in range(len(x_))]\n",
    "# [np.mean(dict_l[i]) for i in dict_l.keys()]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
